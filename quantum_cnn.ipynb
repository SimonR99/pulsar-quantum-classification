{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import specificity_score, negative_prediction_value_score, gmean_score, informedness_score\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from base_pulsa_ai import BasePulsarAI\n",
    "import pennylane as qml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pulsar_ai = BasePulsarAI()\n",
    "x_train, x_test, y_train, y_test = base_pulsar_ai.get_torch_htru_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "wires = range(8)\n",
    "\n",
    "def conv(params, wires):\n",
    "    qml.RY(params[0], wires=wires[0])\n",
    "    qml.RY(params[1], wires=wires[1])\n",
    "    qml.CNOT(wires=[wires[0], wires[1]])\n",
    "    \n",
    "def conv_layer(params, stride, n_qubits):\n",
    "    for i in range(n_qubits):\n",
    "        conv(params[2*i:2*i+2], [wires[i], wires[(i+stride)%n_qubits]])\n",
    "\n",
    "n_qubits = 8\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "n_layers = 6\n",
    "weight_shapes = {\"weights\": (n_layers, n_qubits)}\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def qnode(inputs, weights):\n",
    "    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
    "    #conv_layer(weights, 5, 8)\n",
    "\n",
    "    #qml.Barrier()\n",
    "    #conv_layer(weights, 3, 4)\n",
    "\n",
    "    #qml.Barrier()\n",
    "\n",
    "    return qml.expval(qml.PauliZ(wires=n_qubits-1))\n",
    "\n",
    "\n",
    "random_input = np.random.randn(8)\n",
    "weights = np.random.randn(16)\n",
    "\n",
    "#qml.draw_mpl(qnode)(random_input, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        ...,\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "class QuantumCNN(nn.Module):\n",
    "    def __init__(self, num_features, output_dim):\n",
    "        super(QuantumCNN, self).__init__()\n",
    "        self.qlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 8, 1)\n",
    "        \n",
    "        x = self.qlayer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "input_dim  = 8 # number of features\n",
    "output_dim = 2 # binary classification, to be change for sigmoid ?\n",
    "\n",
    "\n",
    "model = QuantumCNN(input_dim, output_dim)\n",
    "output_train = model(x_train)\n",
    "\n",
    "print(output_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     36\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m---> 38\u001b[0m \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_losses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Predict the test set\u001b[39;00m\n\u001b[0;32m     41\u001b[0m output_test \u001b[38;5;241m=\u001b[39m model(x_test)\n",
      "Input \u001b[1;32mIn [47]\u001b[0m, in \u001b[0;36mtrain_network\u001b[1;34m(model, optimizer, criterion, x_train, y_train, x_test, y_test, num_epochs, train_losses, test_losses)\u001b[0m\n\u001b[0;32m     12\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m criterion(output_train, y_train)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#backward propagation: calculate gradients\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mloss_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#update the weights\u001b[39;00m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\simon\\.conda\\envs\\quantum\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\simon\\.conda\\envs\\quantum\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ss = ShuffleSplit(n_splits=base_pulsar_ai.num_runs, train_size=60, test_size=120)\n",
    "\n",
    "def train_network(model,optimizer,criterion,x_train,y_train,x_test,y_test,num_epochs,train_losses,test_losses):\n",
    "    for epoch in range(num_epochs):\n",
    "        #clear out the gradients from the last step loss.backward()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward feed\n",
    "        output_train = model(x_train)\n",
    "\n",
    "        #calculate the loss\n",
    "        loss_train = criterion(output_train, y_train)\n",
    "        \n",
    "        #backward propagation: calculate gradients\n",
    "        loss_train.backward()\n",
    "\n",
    "        #update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        output_test = model(x_test)\n",
    "        loss_test = criterion(output_test,y_test)\n",
    "\n",
    "        train_losses[epoch] = loss_train.item()\n",
    "        test_losses[epoch] = loss_test.item()\n",
    "\n",
    "        print(f\"Epoch {epoch} train loss: {loss_train.item()} test loss: {loss_test.item()}\")\n",
    "\n",
    "for train_index, test_index in ss.split(x_train):\n",
    "    num_epochs = base_pulsar_ai.max_num_epochs\n",
    "    train_losses = np.zeros(num_epochs)\n",
    "    test_losses  = np.zeros(num_epochs)\n",
    "    model = QuantumCNN(input_dim, output_dim)\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "    train_network(model,optimizer,criterion, x_train,y_train, x_test,y_test,num_epochs,train_losses,test_losses)\n",
    "\n",
    "    # Predict the test set\n",
    "    output_test = model(x_test)\n",
    "    _, predicted_test = torch.max(output_test, 1)\n",
    "\n",
    "    # Calculate the scores\n",
    "    base_pulsar_ai.append_score(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the scores\n",
    "for metric, values in base_pulsar_ai.scores.items():\n",
    "    mean_value = np.mean(values)\n",
    "    std_value = np.std(values)\n",
    "    print(f\"{metric.capitalize()}: {mean_value:.3f} ± {std_value:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
